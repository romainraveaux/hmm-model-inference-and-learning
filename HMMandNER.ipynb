{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model (HMM) for Named-Entity  Recognition (NER) : Model, inference and learning\n",
    "\n",
    "\n",
    "# Goals : \n",
    "1. A presentation of HMM\n",
    "2. Formal definitions on inference and learning with HMM\n",
    "3. An application Named-Entity  Recognition (NER)\n",
    "\n",
    "# Author: Romain Raveaux (romain.raveaux@univ-tours.fr)\n",
    "\n",
    "# The lecture\n",
    "The content of the is notebook is based on the following lectures: \n",
    "Supervised Machine Learning for structured input/output: \n",
    "\n",
    "*   1\\. Introduction to supervised Machine Learning: A probabilistic introduction [PDF](http://romain.raveaux.free.fr/document/courssupervisedmachinelearningRaveaux.pdf)\n",
    "\n",
    "*   2\\. **Connecting local models : The case of chains** [PDF slides](http://romain.raveaux.free.fr/document/Connecting%20local%20models%20the%20case%20of%20chains%20.pdf)\n",
    "\n",
    "*   3\\. Connecting local models : Beyond chains and trees.[PDF slides](http://romain.raveaux.free.fr/document/Structured%20Output%20Learning.pdf)\n",
    "\n",
    "*   4\\. Machine Learning and Graphs : Introduction and problems [PDF slides](http://romain.raveaux.free.fr/document/cours%20IA%20DI5%20graphs%20introV2.pdf)\n",
    "\n",
    "*   5\\. Graph Neural Networks. [PDF slides](http://romain.raveaux.free.fr/document/graph%20neural%20networks%20romain%20raveaux.pdf)\n",
    "\n",
    "*   6\\. Graph Kernels. [PDF slides](http://romain.raveaux.free.fr/document/graph%20kernel%20romain%20raveaux.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity  Recognition (NER)\n",
    "\n",
    "Definition taken from Wikipedia (https://en.wikipedia.org/wiki/Named-entity_recognition).\n",
    "\n",
    "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Most research on NER systems has been structured as taking an unannotated block of text, such as this one:\n",
    "\n",
    "Jim bought 300 shares of Acme Corp. in 2006.\n",
    "\n",
    "And producing an annotated block of text that highlights the names of entities:\n",
    "\n",
    "[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\n",
    "\n",
    "In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition : \n",
    "$\\mathcal{D}= \\{x^{(j)},y^{(j)}\\}_{j=1}^N$ with $N$ the number of pair samples.\n",
    "\n",
    "$x^{(j)} \\in \\mathcal{S}^{M \\times 1}$ is a sequence of $M$ words. A word takes its value in the set $\\mathcal{S}$.\n",
    "\n",
    "$\\mathcal{S}$ is the set of all the words. $\\mathcal{S}$ can be called a dictionary.\n",
    "\n",
    "$x^{(j)}_i \\in \\mathcal{S}$ is a word. $x_i$ for short when there is no need to specify the index of the sequence.\n",
    "\n",
    "Let us define $D=|\\mathcal{S}|$. $D$ is the size of the dictionary. The number of possible words.\n",
    "\n",
    "$y^{(j)} \\in \\mathcal{E}^{M \\times 1}$ is a sequence of named-entities. A Named-entity takes its value in the set $\\mathcal{E}$. \n",
    "\n",
    "$\\mathcal{E}$ is the set of all the named-entities. \n",
    "\n",
    "$y^{(j)}_i \\in \\mathcal{E}$ is a named-entity. $y_i$ for short when there is no need to specify the index of the sequence.\n",
    "\n",
    "Let us define $K=|\\mathcal{E}|$. $K$ is the size of the named-entity set. The number of named-entities.\n",
    "\n",
    "### We want to find a function $f: \\mathcal{S}^{M \\times 1} \\to \\mathcal{E}^{M \\times 1}$.\n",
    "\n",
    "\n",
    "# A sequence model : HMM\n",
    "Let us try to model our sequence thanks to a HMM model. \n",
    "![Graph Convolution](http://romain.raveaux.free.fr/document/hmmmodel.PNG)\n",
    "\n",
    "# Inference with HMM : Maximum a posteriori\n",
    "We take a new set of measurements ($x^{new}$) and use the model to tell us about the world state ($y$).\n",
    "The HMM is generative model because it models $Pr(x_i|y_i)$ where we want $Pr(y_i|x_i)$.\n",
    "Inference with a generative model can be achieved by the baye‚Äôs rule. We obtain the so called posterior distribution. \n",
    "![Graph Convolution](http://romain.raveaux.free.fr/document/MAPHMM.PNG)\n",
    "Finding the maximum of the posterior distribution is known as Maximum a posteriori (**MAP**).\n",
    "\n",
    "# Inference with HMM : the optimization problem\n",
    "Let us define the optimization problem to find the MAP.\n",
    "$$Pr(y_1,\\cdots,y_M|x^{new}_1,\\cdots,x^{new}_M)=\\frac{Pr(x_1^{new},\\cdots,x_M^{new}|y_1,\\cdots,y_M).Pr(y_1,\\cdots,y_M)}{Pr(x_1^{new},\\cdots,x_M^{new})}$$\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\max_{y_1,\\cdots,y_M} \\bigg[Pr(y_1,\\cdots,y_M|x^{new}_1,\\cdots,x^{new}_M) \\bigg]$$\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\max_{y_1,\\cdots,y_M} \\bigg[ Pr(x_1^{new},\\cdots,x_M^{new}|y_1,\\cdots,y_M).Pr(y_1,\\cdots,y_M) \\bigg]$$\n",
    "\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\max_{y_1,\\cdots,y_M} \\bigg[ \\prod_{i=1}^M Pr(x_i^{new}|y_i) .Pr(y_1)\\prod_{i=2}^M Pr(y_i|y_{i-1}) \\bigg]$$\n",
    "\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\min_{y_1,\\cdots,y_M} - \\log \\bigg[ Pr(x_1^{new},\\cdots,x_M^{new}|y_1,\\cdots,y_M).Pr(y_1,\\cdots,y_M) \\bigg]$$\n",
    "\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\min_{y_1,\\cdots,y_M} \\bigg[ -\\sum_{i=1}^M Pr(x_i^{new}|y_i) - Pr(y_1) - \\sum_{i=2}^M Pr(y_i|y_{i-1}) \\bigg]$$\n",
    "$$\\hat{y_1},\\cdots,\\hat{y_M}=arg \\min_{y_1,\\cdots,y_M} \\bigg[  \\sum_{i=1}^M U_i(y_i) + \\sum_{i=2}^M P_i(y_i,y_{i-1}) \\bigg]$$\n",
    "$$U_i(y_i) =-\\log [ Pr(x_i^{new}|y_i) ]$$\n",
    "$$P_i(y_i,y_{i-1}) =-\\log [ Pr(y_i|y_{i-1}) ]$$\n",
    "\n",
    "# Solving the optimization problem\n",
    "The MAP problem can be solved thanks to the Viterbi Algorithm which complexity is $O(MK^2)$ where $K$ is the number of states for $y_{i,j}$ variables. \n",
    "\n",
    "## Viterbi Algorithm\n",
    "The algorithm' concept :\n",
    "$S_{1,k}=U_1(y_1=k,x^{new}_1)$\n",
    "\n",
    "$S_{2,k}=U_2(y_2=k,x^{new}_2)+ \\min_l [S_{1,l}+P_2(y_1=l,y_2=k)] $\n",
    "\n",
    "$S_{i,k}=U_i(y_i=k,x^{new}_i)+ \\min_l [S_{i-1,l}+P_i(y_{i-1}=l,y_i=k)] $\n",
    "\n",
    "$\\hat{y}_{M}=argmin_k [S_{M,k}] $\n",
    "\n",
    "# Learning with HMM\n",
    "##  Modeling the distributions\n",
    "So far there is no learning.\n",
    "1. We just give a sequence (x) and output the labeled sequence (y)\n",
    "2. Where learning can be introduced ?\n",
    "3. Where are the parameters ?\n",
    "\n",
    "Supervised learning\n",
    "1. Relatively simple. We first isolate the part of the model that we want to learn : $Pr(x_i|y_i;\\lambda)$ and $Pr(y_i|y_{i-1};\\gamma)$. For example, we might learn the parameters $\\lambda$, $\\gamma$ from paired examples of $x_i$ and $y_i$. \n",
    "\n",
    "2. We model $Pr(x_i|y_i)$ as being categorically distributed, where the parameters depend $y_i$, so that $Pr(x_i|y_i)=Cat_{x_i}[\\lambda]$. Categorical distribution is well suited for words because words are by definition categorial values.  $\\lambda \\in [0, 1]^{K \\times D}$. \n",
    "\n",
    "3. We model $Pr(y_i|y_{i-1})$ as being categorically distributed, where the parameters depend on the previous sign $y_{i-1}$\n",
    "so that $Pr(y_i|y_{i-1}=k)=Cat_{y_i}[\\gamma]$.  In our case, $\\gamma \\in [0, 1]^{K \\times K}$.\n",
    "\n",
    "### Recall on the categorical distribution\n",
    "$y_i \\in \\mathcal{E}$ is a named-entity. K=$|\\mathcal{E}|$ is the number of named-entities.\n",
    "$Pr(y_i)=Cat_{y_i}[\\theta]$\n",
    "The probabilities of observing the K outcomes are held in a $K \\times 1$ parameter vector $\\theta = [ \\theta_1, \\theta_2, \\cdots, \\theta_K]$, where $\\theta_k \\in [0, 1]$ and $\\sum_{k=1}^K \\theta_k =1$\n",
    "\n",
    "The categorical distribution can be visualized as a normalized histogram with K bins and can be\n",
    "written as\n",
    "\n",
    "$Pr(y_i = k) = \\theta_k $\n",
    "\n",
    "For short, we use the notation\n",
    "\n",
    "$Pr(y_i) = Cat_{y_i}[\\theta]$\n",
    "\n",
    "Alternatively, we can think of the data as taking values $y_i \\in \\{e^{(1)}, \\cdots, e^{(k)} \\}$ where $e^{(k)}$ is the $k^{th}$ unit vector; all elements of $e^{(k)}$ are zero except the $k^{th}$, which is one (i.e. $e^{(2)} = [0,1,0,0,0]$ and $e^{(2)}_1=0$). \n",
    "\n",
    "Here we can write:\n",
    "\n",
    "$Pr(y_i = e^{(k)}) = \\prod_{j=1}^K \\theta_j^{e_j^{(k)}}=\\theta_k$\n",
    "where $e_j^{(k)}$ is the $j^{th}$ element of $e^{(k)}$.\n",
    "\n",
    "### A non-positional model\n",
    "We model $Pr(x_i|y_i)$ as we would do it in Natural Language Processing or Computer Vision fields. It means that we are thinking in terms of a time invariant model. $Pr(x_i|y_i)$ does not depends on the time step.\n",
    "\n",
    "\n",
    "### Limitations of the HMM : \n",
    "1. Markov Assumption : The future depends only on the present. Also known as firs order Markov chain\n",
    "2. It is a generative model. We want to predict $y_i$ and instead we generate $x_i$\n",
    "3. $Pr(x_i|y_i)$ and $Pr(y_i|y_{i-1})$ holds parameters that are shared through time. All time steps have the same parameters. Corresponding to the assumption of a stationary time series.\n",
    "\n",
    "## Learning HMM by Maximum Likelihood\n",
    "Let us denote $W= \\{\\lambda, \\gamma \\}$\n",
    "\n",
    "\n",
    "1 . Let us express the joint probability of the data set ùíü . For one input and one output, the HMM model is :\n",
    "$$Pr(y_1,\\cdots,y_M;x_1,\\cdots,x_M)=Pr(x_1,\\cdots,x_M|y_1,\\cdots,y_M).Pr(y_1,\\cdots,y_M)$$\n",
    "$$Pr(y_1,\\cdots,y_M;x_1,\\cdots,x_M|W)= \\bigg[ \\prod_{i=1}^M Pr(x_i|y_i,\\lambda) \\bigg] \\bigg[ Pr(y_1)\\prod_{i=2}^M Pr(y_i|y_{i-1}, \\gamma)\\bigg] $$\n",
    "\n",
    "2 . For the whole data set : Assuming each data sequence was drawn independently from the distribution (Independent and identically distributed (i.i.d) ) !!!\n",
    "\n",
    "$$Pr(\\mathcal{D}|W)=  Pr(y^{(1)},\\cdots,y^{(N)};x^{(1)},\\cdots,x^{(N)}|W)$$\n",
    "$$Pr(\\mathcal{D}|W)= \\prod_{j=1}^N\\bigg[Pr(y^{(j)}_1,\\cdots,y^{(j)}_M;x^{(j)}_1,\\cdots,x^{(j)}_M|W)\\bigg]$$\n",
    "$$Pr(\\mathcal{D}|W)= \\prod_{j=1}^N\\bigg[ \\prod_{i=1}^M \\bigg( Pr(x^{(j)}_i|y^{(j)}_i,\\lambda) \\bigg)  Pr(y^{(j)}_1)\\prod_{i=2}^M \\bigg( Pr(y^{(j)}_i|y^{(j)}_{i-1}, \\gamma)\\bigg)\\bigg] $$\n",
    "$$\\hat{W}=arg \\max_W  Pr(\\mathcal{D}|W) $$\n",
    "$$\\hat{W}=arg \\max_W   \\prod_{j=1}^N\\bigg[\n",
    "Pr(y^{(j)}_1,\\cdots,y^{(j)}_M;x^{(j)}_1,\\cdots,x^{(j)}_M|W)\\bigg] $$\n",
    "\n",
    "3 . Let us fit the probability model by the maximum of likelihood\n",
    "\n",
    "$$\\hat{W}=arg \\max_W  \\prod_{j=1}^N\\bigg[ \\prod_{i=1}^M \\bigg( Pr(x^{(j)}_i|y^{(j)}_i,\\lambda) \\bigg)  Pr(y^{(j)}_1)\\prod_{i=2}^M \\bigg( Pr(y^{(j)}_i|y^{(j)}_{i-1}, \\gamma)\\bigg)\\bigg] $$\n",
    "\n",
    "$$\\hat{\\lambda}=arg \\max_{\\lambda}  \\prod_{j=1}^N\\bigg[ \\prod_{i=1}^M \\bigg( Pr(x^{(j)}_i|y^{(j)}_i,\\lambda) \\bigg)  \\bigg]$$\n",
    "\n",
    "\n",
    "$$\\hat{ \\gamma}=arg \\max_{ \\gamma}  \\prod_{j=1}^N\\bigg[ Pr(y^{(j)}_1)\\prod_{i=2}^M \\bigg( Pr(y^{(j)}_i|y^{(j)}_{i-1}, \\gamma)\\bigg)  \\bigg]$$\n",
    "\n",
    "## Maximum Likelihood of a categorical distribution\n",
    "We estimate the parameters $\\lambda=[0,1]^{K \\times D}$. $\\lambda$ is a matrix. $\\lambda_k$ is a vector $\\lambda_k=[0,1]^{1 \\times D}$ representing the word distribution for a given named-entity $Pr(x_i|y_i=k)=\\lambda_k$.\n",
    "\n",
    "To find the maximum likelihood solution, we maximize the product of the likelihoods for each individual data point with respect to the parameters $\\lambda$.\n",
    "\n",
    "$\\hat{\\lambda_k}= arg \\max_{\\lambda_k \\in [0, 1]^{D}}  \\prod_{j=1}^N\\bigg[ \\prod_{i=1}^M \\bigg( Pr(x^{(j)}_i|y^{(j)}_i=k,\\lambda_k) \\bigg)  \\bigg] \\text{  s.t.  }  \\sum_{d=1}^D \\lambda_{k,d} =1$\n",
    "\n",
    "\n",
    "We set the derivatives equal to zero, and solve for $\\lambda_{k,d}$ to obtain :\n",
    "$\\hat{\\lambda_{k,d}}=\\frac{N_d}{\\sum_{l=1}^{D} N_l}=\\frac{Pr(x=d,y=k)}{Pr(y=k)}$\n",
    "\n",
    "$N_d$ = For a given named-entity (k), the number of time we observe the word $d$.\n",
    "\n",
    "$\\sum_{l=1}^{D} N_l$ = For a given named-entity (k), the sum of the number of time we observe the word $l$.\n",
    "\n",
    "In other words, $\\lambda_{k,d}$ is the proportion of times that we observed the bin $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "\n",
    "We need a data set. We will use the CoNLL-2003 databse.\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\n",
    "\n",
    "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1.\n",
    "\n",
    "For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n",
    "\n",
    "\n",
    "\n",
    "WORD      POS   CHUNK NE\n",
    "\n",
    "U.N.      NNP   I-NP  I-ORG\n",
    "\n",
    "official  NN    I-NP  O\n",
    "\n",
    "Ekeus     NNP   I-NP  I-PER\n",
    "\n",
    "heads     VBZ   I-VP  O\n",
    "\n",
    "for       IN    I-PP  O\n",
    "\n",
    "Baghdad   NNP   I-NP  I-LOC\n",
    "\n",
    ".         .     O     O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n‚Äôest pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex√©cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.deepai.org/conll2003.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us get in touch with data \n",
    "### Let us parse the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences:  14987\n",
      "A sentence:  [('EU', 'NNP', 'I-ORG'), ('rejects', 'VBZ', 'O'), ('German', 'JJ', 'I-MISC'), ('call', 'NN', 'O'), ('to', 'TO', 'O'), ('boycott', 'VB', 'O'), ('British', 'JJ', 'I-MISC'), ('lamb', 'NN', 'O'), ('.', '.', 'O')]\n",
      "A word and the POS tag and the NER tag :  ('EU', 'NNP', 'I-ORG')\n",
      "POS = Part of Speech\n"
     ]
    }
   ],
   "source": [
    "# Let us parse the data \n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "train = ConllCorpusReader('./dataset', 'eng.train', ['words','pos','ne','chunk'])\n",
    "allsentences=train.iob_sents()\n",
    "print(\"The number of sentences: \",len(allsentences))\n",
    "print(\"A sentence: \",allsentences[0])\n",
    "print(\"A word and the POS tag and the NER tag : \",allsentences[0][0])\n",
    "print(\"POS = Part of Speech\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us compute K the number of NER signs and D the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 8\n",
      "D= 23623\n",
      "Number of words= 203621\n",
      "{'B-LOC': 0, 'B-MISC': 1, 'I-PER': 2, 'I-ORG': 3, 'I-LOC': 4, 'O': 5, 'I-MISC': 6, 'B-ORG': 7}\n"
     ]
    }
   ],
   "source": [
    "setne=set([])\n",
    "setword=set([])\n",
    "nbwords=0\n",
    "for i in range(len(allsentences)):\n",
    "    onesentence=allsentences[i]\n",
    "    for j in range(len(onesentence)):\n",
    "        onetoken=onesentence[j]\n",
    "        word=onetoken[0]\n",
    "        pos=onetoken[1]\n",
    "        ne=onetoken[2]\n",
    "        #print(word)\n",
    "        #print(pos)\n",
    "        #print(ne)\n",
    "        setword.add(word)\n",
    "        setne.add(ne)\n",
    "        nbwords=nbwords+1\n",
    "\n",
    "\n",
    "K=len(setne)\n",
    "D=len(setword)\n",
    "print(\"K=\",K)\n",
    "print(\"D=\",D)\n",
    "print(\"Number of words=\",nbwords)\n",
    "\n",
    "dicne={}\n",
    "for ne in setne:\n",
    "    dicne[ne]=len(dicne)\n",
    "\n",
    "print(dicne)\n",
    "\n",
    "dicword={}\n",
    "for word in setword:\n",
    "    dicword[word]=len(dicword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us compute the probability distribution $Pr(y_i)=Cat_{x_i}[\\theta]$.  $\\theta \\in [0,1]^{1 \\times K}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10000e+01 3.50000e+01 1.11280e+04 1.00010e+04 8.28600e+03 1.69578e+05\n",
      "  4.55800e+03 2.40000e+01]]\n",
      "We should normalize the thetaa to sum to one but I need it not normalize Pr(x_i|y_i) later\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "thetaa=np.zeros((1,K))\n",
    "for i in range(len(allsentences)):\n",
    "    onesentence=allsentences[i]\n",
    "    for j in range(len(onesentence)):\n",
    "        onetoken=onesentence[j]\n",
    "        word=onetoken[0]\n",
    "        pos=onetoken[1]\n",
    "        ne=onetoken[2]\n",
    "        indexne=dicne[ne]\n",
    "        thetaa[0,indexne]=thetaa[0,indexne]+1\n",
    "        \n",
    "print(thetaa)\n",
    "print(\"We should normalize the thetaa to sum to one but I need it not normalize Pr(x_i|y_i) later\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us compute the probability distribution $Pr(x_i|y_i)=Cat_{x_i}[\\lambda]$.  $\\lambda \\in [0,1]^{K \\times D}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "lambdaa=np.zeros((K,D))\n",
    "for i in range(len(allsentences)):\n",
    "    onesentence=allsentences[i]\n",
    "    for j in range(len(onesentence)):\n",
    "        onetoken=onesentence[j]\n",
    "        word=onetoken[0]\n",
    "        pos=onetoken[1]\n",
    "        ne=onetoken[2]\n",
    "        indexne=dicne[ne]\n",
    "        indexword=dicword[word]\n",
    "        lambdaa[indexne,indexword]=lambdaa[indexne,indexword]+1\n",
    "\n",
    "\n",
    "for k in range(K):\n",
    "    lambdaa[k,:]/=thetaa[0,k]\n",
    "print(lambdaa.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us compute the probability distribution $Pr(y_i|y_{i-1}=k)=Cat_{y_i}[\\gamma]$.  $\\gamma \\in \\mathbb{R}^{K \\times K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "gammaa=np.zeros((K,K))\n",
    "\n",
    "for i in range(len(allsentences)):\n",
    "    onesentence=allsentences[i]\n",
    "    for j in range(len(onesentence)-1):\n",
    "        onetoken=onesentence[j]\n",
    "        ne=onetoken[2]\n",
    "        indexnejminusone=dicne[ne]\n",
    "        \n",
    "        onetoken=onesentence[j+1]\n",
    "        ne=onetoken[2]\n",
    "        indexnej=dicne[ne]\n",
    "        gammaa[indexnejminusone,indexnej]=gammaa[indexnejminusone,indexnej]+1\n",
    "\n",
    "for k in range(0,K):\n",
    "    gammaa[k,:]=gammaa[k,:]/gammaa[k,:].sum()\n",
    "print(gammaa.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us code the Viterbi Algorithm\n",
    "\n",
    "The algorithm' concept :\n",
    "$S_{1,k}=U_1(y_1=k,x^{new}_1)$\n",
    "\n",
    "$S_{2,k}=U_2(y_2=k,x^{new}_2)+ \\min_l [S_{1,l}+P_2(y_1=l,y_2=k)] $\n",
    "\n",
    "$S_{i,k}=U_i(y_i=k,x^{new}_i)+ \\min_l [S_{i-1,l}+P_i(y_{i-1}=l,y_i=k)] $\n",
    "\n",
    "$\\hat{y}_{M}=argmin_k [S_{M,k}] $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Ui(yi,x):\n",
    "    epsilon = 0.0000000001\n",
    "    pr=-np.log(lambdaa[yi,x]+epsilon)\n",
    "    return pr\n",
    "\n",
    "def Pi(yi,yiminusone):\n",
    "    epsilon = 0.0000000001\n",
    "    return -np.log(gammaa[yiminusone,yi]+epsilon)\n",
    "\n",
    "def FindMin(S,yi,i):\n",
    "    minindex=-1\n",
    "    minval=999999\n",
    "    for l in range(0,K):\n",
    "        vals=S[i-1,l]\n",
    "        valp=Pi(yi,l)\n",
    "        val=vals+valp\n",
    "        if val < minval:\n",
    "            minval=val\n",
    "            minindex=l\n",
    "    return minval,minindex\n",
    "\n",
    "def Viterbi(xnew):\n",
    "    sentencelength=len(xnew)\n",
    "\n",
    "    S=np.zeros((sentencelength,K))\n",
    "    Sindice=np.zeros((sentencelength,K))\n",
    "    i=0\n",
    "    x=xnew[i]\n",
    "    for yi in range(0,K):\n",
    "        S[i,yi]=Ui(yi,x)\n",
    "    \n",
    "    for i in range(1,sentencelength):\n",
    "        x=xnew[i]\n",
    "        for yi in range(0,K):\n",
    "            minvall,minl=FindMin(S,yi,i)\n",
    "            Sindice[i,yi]=minl\n",
    "            S[i,yi]=Ui(yi,x)+minvall\n",
    "\n",
    "    ypred=[]\n",
    "    ycur=S[sentencelength-1,:].argmin()\n",
    "    ypred.append(ycur)\n",
    "    i=sentencelength-1\n",
    "    while i >= 1:\n",
    "        ycur=Sindice[i,int(ycur)]\n",
    "        ypred.append(int(ycur))\n",
    "        i=i-1\n",
    "    ypred.reverse()\n",
    "    return ypred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us try the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', 'NNP', 'I-ORG'), ('rejects', 'VBZ', 'O'), ('German', 'JJ', 'I-MISC'), ('call', 'NN', 'O'), ('to', 'TO', 'O'), ('boycott', 'VB', 'O'), ('British', 'JJ', 'I-MISC'), ('lamb', 'NN', 'O'), ('.', '.', 'O')]\n",
      "Let us get only the words\n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Let us transform the words to integers\n",
      "[19162, 2832, 22573, 12362, 17509, 21542, 6855, 3829, 15450]\n",
      "Let us call Viterbi\n",
      "[3, 5, 6, 5, 5, 5, 6, 5, 5]\n",
      "Let us get only the named-entities\n",
      "['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n",
      "Let us transform thenamed-entities to integers\n",
      "[3, 5, 6, 5, 5, 5, 6, 5, 5]\n",
      "Let us compare y and ypred by a ratio of similarity\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "onesentence=allsentences[0]\n",
    "print(onesentence)\n",
    "print(\"Let us get only the words\")\n",
    "words=[]\n",
    "for mytuple in onesentence:\n",
    "    words.append(mytuple[0])\n",
    "print(words)\n",
    "print(\"Let us transform the words to integers\")\n",
    "wordsinteger=[]\n",
    "for word in words:\n",
    "    wordsinteger.append(dicword[word])\n",
    "print(wordsinteger)\n",
    "\n",
    "print(\"Let us call Viterbi\")\n",
    "ypred=Viterbi(wordsinteger)\n",
    "print(ypred)\n",
    "\n",
    "\n",
    "print(\"Let us get only the named-entities\")\n",
    "ywords=[]\n",
    "for mytuple in onesentence:\n",
    "    ywords.append(mytuple[2])\n",
    "print(ywords)\n",
    "print(\"Let us transform thenamed-entities to integers\")\n",
    "y=[]\n",
    "for yword in ywords:\n",
    "    y.append(dicne[yword])\n",
    "print(y)\n",
    "\n",
    "print(\"Let us compare y and ypred by a ratio of similarity\")\n",
    "score=np.sum(np.array(y)==np.array(ypred))/len(y)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us write some helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentencetoword(onesentence):\n",
    "    words=[]\n",
    "    for mytuple in onesentence:\n",
    "        words.append(mytuple[0])\n",
    "    return words\n",
    "\n",
    "def wordstointegers(words):\n",
    "    wordsinteger=[]\n",
    "    for word in words:\n",
    "        wordsinteger.append(dicword[word])\n",
    "    return wordsinteger\n",
    "\n",
    "def sentencetoNE(onesentence):\n",
    "    ywords=[]\n",
    "    for mytuple in onesentence:\n",
    "        ywords.append(mytuple[2])\n",
    "    return ywords\n",
    "\n",
    "def NEtointegers(ywords):\n",
    "    y=[]\n",
    "    for yword in ywords:\n",
    "        y.append(dicne[yword])\n",
    "    return y\n",
    "\n",
    "def similaritymeasure(y,ypred):\n",
    "    score=np.sum(np.array(y)==np.array(ypred))/len(y)\n",
    "    return score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try our method on all the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811974191082682\n"
     ]
    }
   ],
   "source": [
    "allscore=0\n",
    "count=0\n",
    "for i in range(len(allsentences)):\n",
    "    #print(i)\n",
    "    onesentence=allsentences[i]\n",
    "    \n",
    "    if(len(onesentence)>1):    \n",
    "        words=sentencetoword(onesentence)\n",
    "        wordsinteger=wordstointegers(words)\n",
    "\n",
    "        ywords=sentencetoNE(onesentence)\n",
    "        y=NEtointegers(ywords)\n",
    "\n",
    "\n",
    "        ypred=Viterbi(wordsinteger)\n",
    "        score=similaritymeasure(y,ypred)\n",
    "        allscore+=score\n",
    "        count=count+1\n",
    "\n",
    "allscore/=count\n",
    "print(allscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get 98% of good named-entity recognition on the training data set. !!!! Well done\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "1. We have implemented a Hidden Markov Model for named-entity recognition.\n",
    "2. The learning phase is achieved by modeling probability distributions as categorical distributions and parameters of these distructions are learned by Maximum Likelihood.\n",
    "3. Inference is achieved by Maximum a posteriori thanks to the Viterbi algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
